import pandas as pd
import numpy as np
import streamlit as st

def load_csv(file):
    """Wczytuje dane z pliku CSV."""
    try:
        # Reset file pointer
        file.seek(0)
        
        # SprÃ³buj rÃ³Å¼nych kodowaÅ„
        encodings = ['utf-8', 'latin-1', 'cp1250', 'iso-8859-1']
        data = None
        
        for encoding in encodings:
            try:
                file.seek(0)
                data = pd.read_csv(file, encoding=encoding, na_values=['?', 'NA', 'na', 'N/A', 'n/a', '', ' ', 'null', 'NULL'])
                st.success(f"âœ… Plik wczytany pomyÅ›lnie (kodowanie: {encoding})")
                break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                st.warning(f"PrÃ³ba z kodowaniem {encoding} nie powiodÅ‚a siÄ™: {str(e)}")
                continue
        
        if data is None:
            st.error("âŒ Nie udaÅ‚o siÄ™ wczytaÄ‡ pliku z Å¼adnym z obsÅ‚ugiwanych kodowaÅ„")
            return None
        
        # Podstawowe czyszczenie danych
        data = clean_data(data)
        
        if data is None or data.empty:
            st.error("âŒ Plik jest pusty lub zawiera tylko nieprawidÅ‚owe dane")
            return None
        
        # Informacje o wczytanych danych
        st.info(f"ğŸ“Š Wczytano: {data.shape[0]} wierszy Ã— {data.shape[1]} kolumn")
        
        # SprawdÅº jakoÅ›Ä‡ danych
        missing_percent = (data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100
        if missing_percent > 0:
            st.warning(f"âš ï¸ BrakujÄ…ce wartoÅ›ci: {missing_percent:.1f}% wszystkich komÃ³rek")
        
        duplicates = data.duplicated().sum()
        if duplicates > 0:
            st.warning(f"âš ï¸ Znaleziono {duplicates} zduplikowanych wierszy")
        
        return data
        
    except Exception as e:
        st.error(f"âŒ BÅ‚Ä…d wczytywania pliku: {str(e)}")
        return None

def clean_data(data):
    """CzyÅ›ci wczytane dane."""
    try:
        if data is None:
            return None
        
        # Skopiuj dane
        cleaned = data.copy()
        
        # UsuÅ„ caÅ‚kowicie puste wiersze i kolumny
        cleaned = cleaned.dropna(how='all').reset_index(drop=True)
        cleaned = cleaned.dropna(axis=1, how='all')
        
        # ZastÄ…p rÃ³Å¼ne reprezentacje brakujÄ…cych wartoÅ›ci
        missing_representations = ['?', 'NA', 'na', 'N/A', 'n/a', '', ' ', 'null', 'NULL', 'None', 'NONE']
        for col in cleaned.columns:
            for missing_val in missing_representations:
                cleaned[col] = cleaned[col].replace(missing_val, np.nan)
        
        # UsuÅ„ kolumny, ktÃ³re sÄ… w caÅ‚oÅ›ci puste po czyszczeniu
        cleaned = cleaned.dropna(axis=1, how='all')
        
        # SprawdÅº czy zostaÅ‚y jakieÅ› dane
        if cleaned.empty:
            return None
        
        return cleaned
        
    except Exception as e:
        st.error(f"âŒ BÅ‚Ä…d podczas czyszczenia danych: {str(e)}")
        return data

def get_dataset_info(data):
    """Zwraca podstawowe informacje o zbiorze danych."""
    if data is None:
        return None

    # Analiza typÃ³w danych
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
    
    info = {
        "rows": data.shape[0],
        "columns": data.shape[1],
        "columns_names": data.columns.tolist(),
        "numeric_columns": numeric_cols,
        "categorical_columns": categorical_cols,
        "missing_values": data.isna().sum().sum(),
        "duplicated_rows": data.duplicated().sum(),
        "dtypes": data.dtypes,
        "memory_usage": data.memory_usage(deep=True).sum() / 1024**2  # MB
    }

    return info

def detect_column_types(data):
    """Automatycznie wykrywa i sugeruje typy kolumn."""
    if data is None:
        return {}
    
    suggestions = {}
    
    for col in data.columns:
        # SprawdÅº czy kolumna wyglÄ…da na numerycznÄ…
        try:
            pd.to_numeric(data[col], errors='raise')
            suggestions[col] = 'numeric'
            continue
        except:
            pass
        
        # SprawdÅº czy kolumna wyglÄ…da na datÄ™
        try:
            pd.to_datetime(data[col], errors='raise')
            suggestions[col] = 'datetime'
            continue
        except:
            pass
        
        # SprawdÅº czy kolumna wyglÄ…da na boolean
        unique_vals = data[col].dropna().astype(str).str.lower().unique()
        if len(unique_vals) <= 2 and all(val in ['true', 'false', 't', 'f', 'yes', 'no', 'y', 'n', '1', '0', '+', '-'] for val in unique_vals):
            suggestions[col] = 'boolean'
            continue
        
        # SprawdÅº czy kolumna ma maÅ‚Ä… liczbÄ™ unikalnych wartoÅ›ci (kategorie)
        unique_count = data[col].nunique()
        total_count = len(data[col].dropna())
        if unique_count / total_count < 0.05 or unique_count < 10:
            suggestions[col] = 'categorical'
        else:
            suggestions[col] = 'text'
    
    return suggestions

def suggest_data_preprocessing(data):
    """Sugeruje operacje preprocessing na podstawie analizy danych."""
    if data is None:
        return []
    
    suggestions = []
    
    # SprawdÅº brakujÄ…ce wartoÅ›ci
    missing_data = data.isnull().sum()
    total_missing = missing_data.sum()
    
    if total_missing > 0:
        high_missing_cols = missing_data[missing_data > len(data) * 0.5].index.tolist()
        if high_missing_cols:
            suggestions.append(f"RozwaÅ¼ usuniÄ™cie kolumn z wiÄ™cej niÅ¼ 50% brakujÄ…cych wartoÅ›ci: {high_missing_cols}")
        
        medium_missing_cols = missing_data[(missing_data > 0) & (missing_data <= len(data) * 0.5)].index.tolist()
        if medium_missing_cols:
            suggestions.append(f"RozwaÅ¼ wypeÅ‚nienie brakujÄ…cych wartoÅ›ci w kolumnach: {medium_missing_cols}")
    
    # SprawdÅº duplikaty
    duplicates = data.duplicated().sum()
    if duplicates > 0:
        suggestions.append(f"Znaleziono {duplicates} zduplikowanych wierszy - rozwaÅ¼ ich usuniÄ™cie")
    
    # SprawdÅº kolumny z jednÄ… wartoÅ›ciÄ…
    constant_cols = []
    for col in data.columns:
        if data[col].nunique() <= 1:
            constant_cols.append(col)
    
    if constant_cols:
        suggestions.append(f"Kolumny z jednÄ… wartoÅ›ciÄ… (moÅ¼na usunÄ…Ä‡): {constant_cols}")
    
    # SprawdÅº kolumny numeryczne do skalowania
    numeric_cols = data.select_dtypes(include=['number']).columns.tolist()
    if len(numeric_cols) > 1:
        # SprawdÅº rÃ³Å¼nice w skalach
        ranges = {}
        for col in numeric_cols:
            col_range = data[col].max() - data[col].min()
            if not pd.isna(col_range):
                ranges[col] = col_range
        
        if ranges:
            max_range = max(ranges.values())
            min_range = min(ranges.values())
            if max_range / min_range > 100:  # RÃ³Å¼nica wiÄ™ksza niÅ¼ 100x
                suggestions.append("Kolumny numeryczne majÄ… bardzo rÃ³Å¼ne skale - rozwaÅ¼ skalowanie danych")
    
    # SprawdÅº kolumny kategoryczne do kodowania
    categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
    high_cardinality_cols = []
    for col in categorical_cols:
        unique_count = data[col].nunique()
        if unique_count > 10:
            high_cardinality_cols.append(f"{col} ({unique_count} wartoÅ›ci)")
    
    if high_cardinality_cols:
        suggestions.append(f"Kolumny kategoryczne z wysokÄ… kardynalnoÅ›ciÄ…: {high_cardinality_cols}")
    
    return suggestions

def validate_csv_structure(data):
    """Waliduje strukturÄ™ wczytanego CSV."""
    validation = {
        'is_valid': True,
        'errors': [],
        'warnings': [],
        'info': []
    }
    
    if data is None or data.empty:
        validation['is_valid'] = False
        validation['errors'].append("Plik jest pusty")
        return validation
    
    # SprawdÅº rozmiar
    if data.shape[0] < 2:
        validation['warnings'].append("Bardzo maÅ‚a liczba wierszy (mniej niÅ¼ 2)")
    
    if data.shape[1] < 2:
        validation['warnings'].append("Bardzo maÅ‚a liczba kolumn (mniej niÅ¼ 2)")
    
    # SprawdÅº nazwy kolumn
    duplicate_cols = data.columns[data.columns.duplicated()].tolist()
    if duplicate_cols:
        validation['errors'].append(f"Zduplikowane nazwy kolumn: {duplicate_cols}")
        validation['is_valid'] = False
    
    # SprawdÅº procent brakujÄ…cych wartoÅ›ci
    missing_percent = (data.isnull().sum().sum() / (data.shape[0] * data.shape[1])) * 100
    if missing_percent > 80:
        validation['errors'].append(f"Zbyt duÅ¼o brakujÄ…cych wartoÅ›ci: {missing_percent:.1f}%")
        validation['is_valid'] = False
    elif missing_percent > 50:
        validation['warnings'].append(f"DuÅ¼o brakujÄ…cych wartoÅ›ci: {missing_percent:.1f}%")
    elif missing_percent > 0:
        validation['info'].append(f"BrakujÄ…ce wartoÅ›ci: {missing_percent:.1f}%")
    
    # SprawdÅº typy danych
    type_info = detect_column_types(data)
    validation['info'].append(f"Wykryte typy kolumn: {type_info}")
    
    return validation

def get_column_mapping():
    """Zwraca mapowanie oryginalnych nazw kolumn na proponowane nazwy."""
    return {
        'A1': 'PÅ‚eÄ‡',
        'A2': 'Wiek',
        'A3': 'Stosunek_zadÅ‚uÅ¼enia',
        'A4': 'Status_cywilny',
        'A5': 'Typ_klienta_banku',
        'A6': 'Poziom_wyksztaÅ‚cenia',
        'A7': 'BranÅ¼a_zatrudnienia',
        'A8': 'StaÅ¼_zatrudnienia',
        'A9': 'Ma_rachunek_RO',
        'A10': 'Ma_rachunek_OS',
        'A11': 'Liczba_aktywnych_kredytÃ³w',
        'A12': 'Posiada_inne_zobowiÄ…zania',
        'A13': 'Cel_kredytu',
        'A14': 'DÅ‚ugoÅ›Ä‡_historii_kredytowej',
        'A15': 'Roczny_dochÃ³d',
        'A16': 'Decyzja_przyznania_kredytu'
    }
